{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-project n° 2 – Conditional VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listes des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les définir + justifier les valeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les hyperparamètres de notre modèle sont : \n",
    "\n",
    "- batch size = Le nombre d'échantillons sur lesquels le modèle apprend avant de mettre à jour ses paramètres internes (poids du réseau...)\n",
    "- epochs = Le nombre de fois où l'entièreté du jeu de données est utilisée pour l'apprentissage. Une valeur trop faible peut conduire à du sous-apprentissage et au contraire une valeur trop élevée à du surapprentissage\n",
    "- latent_dim = La taille de l'espace latent\n",
    "- learning_rate = Le taux d'apprentissage. Il contrôle la vitesse à laquelle le modèle apprend.\n",
    "- kl_weights = La liste des poids pour la divergence KL (Kullback-Leibler)\n",
    "- beta = Ajuste l'importance de la divergence dans la fonction de perte\n",
    "\n",
    "La fonction de perte utilisée ci-après est définie de la manière suivante :\n",
    "\n",
    "Loss = BCE + beta * KLD\n",
    "\n",
    "Le premier terme est un terme de reconstruction : il mesure à quel point le réseau reconstruit correctement les données d’origine à partir de l’espace latent.\n",
    "Le second terme est un terme de régularisation : il mesure à quel point la distribution de l'espace latent se rapproche d'une loi normale centrée réduite.\n",
    "\n",
    "Ainsi, 3 cas sont à distinguer concernant le paramètre beta :\n",
    "\n",
    "- beta = 1 : On accorde autant d'importance à la reconstruction qu'à la régularisation.\n",
    "- beta > 1 : On donne davantage de poids à la régularisation. On fait en sorte que la distribution de l'espace latent se rapproche au plus d'une gaussienne centrée réduite. Cependant, cela peut diminuer la capacité de reconstruction du modèle.\n",
    "- beta < 1 : On donne davantage de poids à la reconstruction. Le modèle apprend ainsi à capturer plus de détails dans l'image. Cependant, cela peut augmenter le risque de surapprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 30\n",
    "latent_dim = 2\n",
    "learning_rate = 1e-3\n",
    "kl_weights = [1, 10, 100]\n",
    "beta = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:18<00:00, 540367.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 323325.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:05<00:00, 310657.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='../../data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='../../data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un modèle d'AutoEndodeur Variationnel Conditionnel (Conditionnal VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle de VAE conditionnel utilisé par la suite se construit de la manière suivante :\n",
    "\n",
    "- **Un Encodeur** : \n",
    "    - Une couche de convolution composée de 32 filtres, un noyau de taille 4, un stride de 2 and un padding de 1.\n",
    "    - Une couche BatchNorm qui conserve le même nombre de caractérstiques.\n",
    "    - Une fonction d'activation ReLu.\n",
    "    - Une couche de convolution composée de 64 filtres, un noyau de taille 4, un stride de 2 and un padding de 1.\n",
    "    - Une couche BatchNorm qui conserve le même nombre de caractérstiques.\n",
    "    - Une fonction d'activation ReLu.\n",
    "    - Une couche de convolution composée de 128 filtres, un noyau de taille 3, un stride de 2 and un padding de 1..\n",
    "    - Une couche BatchNorm qui conserve le même nombre de caractérstiques.\n",
    "    - Une fonction d'activation ReLu.\n",
    "\n",
    "- **Un Espace Latent** :\n",
    "\n",
    "- **Un Décodeur.** \n",
    "    - Une couche de déconvolution composée de 64 filtres, un noyau de taille 3, un stride de 2 and un padding de 1.\n",
    "    - Une couche BatchNorm qui conserve le même nombre de caractérstiques\n",
    "    - Une fonction d'activation ReLu\n",
    "    - Une couche de déconvolution composée de 32 filtres, un noyau de taille 4, un stride de 2 and un padding de 1.\n",
    "    - Une couche BatchNorm qui conserve le même nombre de caractérstiques\n",
    "    - Une fonction d'activation ReLu\n",
    "    - Une couche de déconvolution composée de 1 filtre, un noyau de taille 4, un stride de 2 and un padding de 1.\n",
    "    - Une fonction d'activation Sigmoïde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Au niveau de l'encodeur :**\n",
    "\n",
    "Pour chacune des couches de convolution :\n",
    "- Un filtre (ou noyau) est une matrice de poids qui parcourt l’image d’entrée pour extraire des caractéristiques spécifiques (de manière générale comme des contours ou des textures). La sortie de la couche est obtenue à partir d'une convolution entre l'objet en entrée et le filtre. Le paramètre kernel_size désigne la taille de la matrice.\n",
    "- Un filtre apprend UNE caractéristique de l’image donc, plus il y a de filtres, plus le modèle retient de caractéristiques.\n",
    "- Le stride indique de combien de pixels le filtre se déplace à chaque étape lorsqu’il parcourt l’image d’entrée (d'abord horizontalement puis verticalement). \n",
    "- Le padding consiste à ajouter des pixels autour de l'image pour essayer de préserver l'information sur les bords de l'image. Un padding de 1 signfie que l'on ajoute une rangée de pixel tout autour de l'image\n",
    "\n",
    "L’augmentation progressive du nombre de filtres dans les couches de convolution permet de capturer des caractéristiques de plus en plus complexes des données d’entrée : les couches proches de l'entrée capturent des motifs de base et au fur et à mesure que l’on avance dans le réseau, on veut que chaque couche représente des détails plus sophistiqués. Avoir plus de filtres permet à chaque couche de détecter des motifs plus variés et spécifiques.\n",
    "\n",
    "La couche BatchNorm permet de normaliser les sorties d'une couche de réseau lors de l'apprentissage.\n",
    "\n",
    "**Au niveau de l'espace latent :**\n",
    "\n",
    "Il est composé de deux couches parallèles dites \"entièrement connectées\" (en d'autres termes tous les neurones sont connectés à ceux de la couche précédente, qui est ici la sortie de l'encodeur E(x)).\n",
    "\n",
    "- La première couche estime un vecteur moyenne µ(x)\n",
    "- La seconde couche estime un vecteur de log-variance $\\log\\sigma^2(x)$\n",
    "\n",
    "Une dernière couche entièrement connectée utilise alors ces informations pour renvoyer le vecteur de l'espace latent z slon une distribution gaussienne : $z(x)\\sim\\mathcal{N}(\\mu(x),Diag(\\sigma(x)))$ \n",
    "\n",
    "**Au niveau du décodeur :**\n",
    "\n",
    "La fonction d'activation utilisée en sortie est la fonction Sigmoïde. Elle renvoie des valeurs comprises entre 0 et 1 , ce qui est utile puisqu' avec ce réseau on cherche à générer des images en noir et blanc. Les résultats obtenus permettent donc directement d'obtenir des nuances de gris.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c sera à expliciter = condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=10):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for mean and log variance\n",
    "        self.fc_mu = nn.Linear(128 * 4 * 4, latent_dim) # (128*4*4) est la taille de sortie de la couche précédente\n",
    "        self.fc_logvar = nn.Linear(128 * 4 * 4, latent_dim) \n",
    "        self.fc_decode = nn.Linear(latent_dim, 128 * 4 * 4) \n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x, c):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.cat((x,c),dim=1)\n",
    "        x = x.view(-1, 128 * 4 * 4) # Flatten the output of the convolutional layers\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "\n",
    "    def latent_sample(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z, c):\n",
    "        x = torch.cat((x,c),dim=1)\n",
    "        x = self.fc_decode(z)\n",
    "        x = x.view(-1, 128, 4, 4)  # Reshape to (128, 4, 4) for the decoder\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.latent_sample(mu, logvar)\n",
    "        return self.decode(z, c), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de la fonction de perte (loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar, beta=1):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum') # terme de reconstruction \n",
    "    # on utilise binary entropy pour forcer les contrastes (sinon on pourrait travailler avec MSE)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # terme de regularisation (on veut se rapprocher loi normale)\n",
    "    return BCE + beta * KLD # l'objectif est d'avoir un bon compromis entre reconstruction et regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage du modèle de VAE conditionnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m recon_batch, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(recon_batch, data, mu, logvar)\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\janca\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\janca\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'c'"
     ]
    }
   ],
   "source": [
    "# Initialize the VAE model and the Adam optimizer\n",
    "vae = ConvVAE(latent_dim=latent_dim)\n",
    "vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model for the given number of epochs\n",
    "# At the end of each epoch, print the training loss\n",
    "for epoch in range(1, epochs + 1):\n",
    "    vae.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch}, Training loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(num_samples=10, digit):\n",
    "# digit est la valeur conditionelle\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Échantillonage selon une loi normale\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)  # Sample random latent vectors\n",
    "\n",
    "        # On ajoute la condition ie la valeur que l'on veut générer\n",
    "        condition = torch.zeros(100,10, dtype=int)\n",
    "        condition[:,digit] = 1 # indique que c'est la valeur digit que l'on cherche à générer\n",
    "\n",
    "        # On génère l'image\n",
    "        samples = vae.decode(z, condition) # Decode the latent vectors\n",
    "        samples = samples.cpu().view(num_samples, 1, 28, 28) # Reshape the samples\n",
    "\n",
    "        fig, ax = plt.subplots(1, num_samples, figsize=(15, 2))\n",
    "        for i in range(num_samples):\n",
    "            ax[i].imshow(samples[i].squeeze(0), cmap='gray')\n",
    "            ax[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "generate_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation de l'espace latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def loss_function(recon_x, x, mu, logvar, beta=1):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + beta * KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and plotting function\n",
    "def train_and_plot(kl_weight):\n",
    "    model = ConvVAE(latent_dims).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        bce_loss = 0\n",
    "        kld_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, mu, logvar = model(data)\n",
    "            loss, bce, kld = loss_function(x_recon, data, mu, logvar, kl_weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            bce_loss += bce.item()\n",
    "            kld_loss += kld.item()\n",
    "        \n",
    "        average_loss = epoch_loss / len(train_loader.dataset)\n",
    "        average_bce = bce_loss / len(train_loader.dataset)\n",
    "        average_kld = kld_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}: Average Loss: {average_loss:.4f}, BCE: {average_bce:.4f}, KLD: {average_kld:.4f}')\n",
    "    \n",
    "    # Plot latent space\n",
    "    plot_latent_space(model, kl_weight)\n",
    "\n",
    "# Function to plot latent space\n",
    "def plot_latent_space(model, kl_weight):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=10000, shuffle=False)\n",
    "        data, labels = next(iter(test_loader))\n",
    "        data = data.to(device)\n",
    "        mu, logvar = model.encode(data)\n",
    "        z = mu  # For visualization, we use the mean\n",
    "        z = z.cpu().numpy()\n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        plt.figure(figsize=(8,6))\n",
    "        scatter = plt.scatter(z[:, 0], z[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "        plt.colorbar(scatter, ticks=range(10))\n",
    "        plt.clim(-0.5, 9.5)\n",
    "        plt.title(f'Latent Space with KL Weight = {kl_weight}')\n",
    "        plt.xlabel('Z1')\n",
    "        plt.ylabel('Z2')\n",
    "        plt.show()\n",
    "\n",
    "# Run training and plotting for different KL weights\n",
    "for kl_weight in kl_weights:\n",
    "    print(f'\\nTraining VAE with KL Weight = {kl_weight}')\n",
    "    train_and_plot(kl_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
